{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "了解如何使用 Sklearn 中的 hyper-parameter search 找出最佳的超參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業\n",
    "請使用不同的資料集，並使用 hyper-parameter search 的方式，看能不能找出最佳的超參數組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.model_selection import cross_val_score\n",
    "data_path = 'data/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train_1.csv', header = None)\n",
    "train_labels = pd.read_csv(data_path + 'trainLabels_1.csv', header = None)\n",
    "test = pd.read_csv(data_path + 'test_1.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 40), (300, 40), (700, 1), (300, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(train,train_labels, test_size = 0.30, random_state = 101)\n",
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 0.8066666666666666\n",
      "KNN 0.9166666666666666\n",
      "Random Forest 0.86\n",
      "Logistic Regression 0.82\n",
      "SVM 0.9033333333333333\n",
      "Decision Tree 0.7233333333333334\n",
      "XGBoost 0.87\n"
     ]
    }
   ],
   "source": [
    "# NAIBE BAYES\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(x_train,y_train.values.ravel())\n",
    "predicted= model.predict(x_test)\n",
    "print('Naive Bayes',accuracy_score(y_test, predicted))\n",
    "\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(x_train,y_train.values.ravel())\n",
    "predicted= knn_model.predict(x_test)\n",
    "print('KNN',accuracy_score(y_test, predicted))\n",
    "\n",
    "#RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n",
    "rfc_model.fit(x_train,y_train.values.ravel())\n",
    "predicted = rfc_model.predict(x_test)\n",
    "print('Random Forest',accuracy_score(y_test,predicted))\n",
    "\n",
    "#LOGISTIC REGRESSION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(solver = 'saga')\n",
    "lr_model.fit(x_train,y_train.values.ravel())\n",
    "lr_predicted = lr_model.predict(x_test)\n",
    "print('Logistic Regression',accuracy_score(y_test, lr_predicted))\n",
    "\n",
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC(gamma = 'auto')\n",
    "svc_model.fit(x_train,y_train.values.ravel())\n",
    "svc_predicted = svc_model.predict(x_test)\n",
    "print('SVM',accuracy_score(y_test, svc_predicted))\n",
    "\n",
    "#DECISON TREE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree_model = DecisionTreeClassifier()\n",
    "dtree_model.fit(x_train,y_train.values.ravel())\n",
    "dtree_predicted = dtree_model.predict(x_test)\n",
    "print('Decision Tree',accuracy_score(y_test, dtree_predicted))\n",
    "\n",
    "#XGBOOST\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train,y_train.values.ravel())\n",
    "xgb_predicted = xgb.predict(x_test)\n",
    "print('XGBoost',accuracy_score(y_test, xgb_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "\n",
    "norm = Normalizer()\n",
    "x_norm_train = norm.fit_transform(x_train)\n",
    "x_norm_test = norm.transform(x_test)\n",
    "norm_train_data = norm.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 0.8\n",
      "Naive Bayes 0.808\n",
      "KNN 0.8633333333333333\n",
      "KNN 0.9019999999999999\n",
      "Random Forest 0.8633333333333333\n",
      "Random Forest 0.8699999999999999\n",
      "Logistic Regression 0.8066666666666666\n",
      "Logistic Regression 0.8220000000000001\n",
      "SVM 0.7866666666666666\n",
      "SVM 0.808\n",
      "Decision Tree 0.79\n",
      "Decision Tree 0.7889999999999999\n",
      "XGBoost 0.86\n",
      "XGBoost 0.8710000000000001\n"
     ]
    }
   ],
   "source": [
    "# NAIBE BAYES\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(x_norm_train,y_train.values.ravel())\n",
    "nb_predicted= nb_model.predict(x_norm_test)\n",
    "print('Naive Bayes',accuracy_score(y_test, nb_predicted))\n",
    "print('Naive Bayes',cross_val_score(nb_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn_model.fit(x_norm_train,y_train.values.ravel())\n",
    "knn_predicted= knn_model.predict(x_norm_test)\n",
    "print('KNN',accuracy_score(y_test, knn_predicted))\n",
    "print('KNN',cross_val_score(knn_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n",
    "rfc_model.fit(x_norm_train,y_train.values.ravel())\n",
    "rfc_predicted = rfc_model.predict(x_norm_test)\n",
    "print('Random Forest',accuracy_score(y_test,rfc_predicted))\n",
    "print('Random Forest',cross_val_score(rfc_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#LOGISTIC REGRESSION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(solver = 'saga')\n",
    "lr_model.fit(x_norm_train,y_train.values.ravel())\n",
    "lr_predicted = lr_model.predict(x_norm_test)\n",
    "print('Logistic Regression',accuracy_score(y_test, lr_predicted))\n",
    "print('Logistic Regression',cross_val_score(lr_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC(gamma = 'auto')\n",
    "svc_model.fit(x_norm_train,y_train.values.ravel())\n",
    "svc_predicted = svc_model.predict(x_norm_test)\n",
    "print('SVM',accuracy_score(y_test, svc_predicted))\n",
    "print('SVM',cross_val_score(svc_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#DECISON TREE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree_model = DecisionTreeClassifier()\n",
    "dtree_model.fit(x_norm_train,y_train.values.ravel())\n",
    "dtree_predicted = dtree_model.predict(x_norm_test)\n",
    "print('Decision Tree',accuracy_score(y_test, dtree_predicted))\n",
    "print('Decision Tree',cross_val_score(dtree_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#XGBOOST\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_norm_train,y_train.values.ravel())\n",
    "xgb_predicted = xgb.predict(x_norm_test)\n",
    "print('XGBoost',accuracy_score(y_test, xgb_predicted))\n",
    "print('XGBoost',cross_val_score(xgb,norm_train_data, train_labels.values.ravel(), cv=10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25054403 0.2055048  0.08026473 0.05033658 0.04895951 0.04489903\n",
      " 0.0417078  0.03127932 0.0230978  0.02100061 0.01619478 0.01264102]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca  = PCA(n_components=12)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "pca_train_data = pca.fit_transform(train)\n",
    "explained_variance = pca.explained_variance_ratio_ \n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 0.8400000000000001\n",
      "KNN 0.907\n",
      "Random Forest 0.9029999999999999\n",
      "Logistic Regression 0.8240000000000001\n",
      "SVM 0.905\n",
      "Decision Tree 0.8029999999999999\n",
      "XGBoost 0.866\n"
     ]
    }
   ],
   "source": [
    "# NAIBE BAYES\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "#nb_model.fit(pca_train_data,y_train.values.ravel())\n",
    "#nb_predicted= nb_model.predict(x_norm_test)\n",
    "#print('Naive Bayes',accuracy_score(y_test, nb_predicted))\n",
    "print('Naive Bayes',cross_val_score(nb_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 5)\n",
    "#knn_model.fit(pca_train_data,y_train.values.ravel())\n",
    "#knn_predicted= knn_model.predict(x_norm_test)\n",
    "#print('KNN',accuracy_score(y_test, knn_predicted))\n",
    "print('KNN',cross_val_score(knn_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n",
    "#rfc_model.fit(pca_train_data,y_train.values.ravel())\n",
    "#rfc_predicted = rfc_model.predict(x_norm_test)\n",
    "#print('Random Forest',accuracy_score(y_test,rfc_predicted))\n",
    "print('Random Forest',cross_val_score(rfc_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#LOGISTIC REGRESSION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(solver = 'saga')\n",
    "#lr_model.fit(pca_train_data,y_train.values.ravel())\n",
    "#lr_predicted = lr_model.predict(x_norm_test)\n",
    "#print('Logistic Regression',accuracy_score(y_test, lr_predicted))\n",
    "print('Logistic Regression',cross_val_score(lr_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC(gamma = 'auto')\n",
    "#svc_model.fit(x_norm_train,y_train.values.ravel())\n",
    "#svc_predicted = svc_model.predict(x_norm_test)\n",
    "#print('SVM',accuracy_score(y_test, svc_predicted))\n",
    "print('SVM',cross_val_score(svc_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#DECISON TREE\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree_model = DecisionTreeClassifier()\n",
    "#dtree_model.fit(x_norm_train,y_train.values.ravel())\n",
    "#dtree_predicted = dtree_model.predict(x_norm_test)\n",
    "#print('Decision Tree',accuracy_score(y_test, dtree_predicted))\n",
    "print('Decision Tree',cross_val_score(dtree_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#XGBOOST\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "#xgb.fit(x_norm_train,y_train.values.ravel())\n",
    "#xgb_predicted = xgb.predict(x_norm_test)\n",
    "#print('XGBoost',accuracy_score(y_test, xgb_predicted))\n",
    "print('XGBoost',cross_val_score(xgb,pca_train_data, train_labels.values.ravel(), cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_all shape : (10000, 40)\n",
      "Random Forest Best Score 0.996\n",
      "Random Forest Best Parmas {'max_depth': 3, 'n_estimators': 10}\n",
      "Random Forest Accuracy 0.9960000000000001\n",
      "KNN Best Score 0.996\n",
      "KNN Best Params {'n_neighbors': 3}\n",
      "KNN Accuracy 0.9960000000000001\n",
      "SVM Best Score 0.996\n",
      "SVM Best Params {'C': 1, 'kernel': 'linear'}\n",
      "SVM Accuracy 0.9960000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "x_all = np.r_[train,test]\n",
    "print('x_all shape :',x_all.shape)\n",
    "\n",
    "# USING THE GAUSSIAN MIXTURE MODEL \n",
    "lowest_bic = np.infty\n",
    "bic = []\n",
    "n_components_range = range(1, 7)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "for cv_type in cv_types:\n",
    "    for n_components in n_components_range:\n",
    "        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n",
    "        gmm.fit(x_all)\n",
    "        bic.append(gmm.aic(x_all))\n",
    "        if bic[-1] < lowest_bic:\n",
    "            lowest_bic = bic[-1]\n",
    "            best_gmm = gmm\n",
    "            \n",
    "best_gmm.fit(x_all)\n",
    "gmm_train = best_gmm.predict_proba(train)\n",
    "gmm_test = best_gmm.predict_proba(test)\n",
    "\n",
    "\n",
    "#Random Forest Classifier\n",
    "rfc = RandomForestClassifier(random_state=99)\n",
    "\n",
    "#USING GRID SEARCH\n",
    "n_estimators = [10, 50, 100, 200,400]\n",
    "max_depth = [3, 10, 20, 40]\n",
    "param_grid = dict(n_estimators=n_estimators,max_depth=max_depth)\n",
    "\n",
    "grid_search_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv = 10,scoring='accuracy',n_jobs=-1).fit(gmm_train, train_labels.values.ravel())\n",
    "rfc_best = grid_search_rfc.best_estimator_\n",
    "print('Random Forest Best Score',grid_search_rfc.best_score_)\n",
    "print('Random Forest Best Parmas',grid_search_rfc.best_params_)\n",
    "print('Random Forest Accuracy',cross_val_score(rfc_best,gmm_train, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#KNN \n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#USING GRID SEARCH\n",
    "n_neighbors=[3,5,6,7,8,9,10]\n",
    "param_grid = dict(n_neighbors=n_neighbors)\n",
    "\n",
    "grid_search_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv = 10, n_jobs=-1,scoring='accuracy').fit(gmm_train,train_labels.values.ravel())\n",
    "knn_best = grid_search_knn.best_estimator_\n",
    "print('KNN Best Score', grid_search_knn.best_score_)\n",
    "print('KNN Best Params',grid_search_knn.best_params_)\n",
    "print('KNN Accuracy',cross_val_score(knn_best,gmm_train, train_labels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#SVM\n",
    "svc = SVC()\n",
    "\n",
    "#USING GRID SEARCH\n",
    "parameters = [{'kernel':['linear'],'C':[1,10,100]},\n",
    "              {'kernel':['rbf'],'C':[1,10,100],'gamma':[0.05,0.0001,0.01,0.001]}]\n",
    "grid_search_svm = GridSearchCV(estimator=svc, param_grid=parameters, cv = 10, n_jobs=-1,scoring='accuracy').fit(gmm_train, train_labels.values.ravel())\n",
    "svm_best = grid_search_svm.best_estimator_\n",
    "print('SVM Best Score',grid_search_svm.best_score_)\n",
    "print('SVM Best Params',grid_search_svm.best_params_)\n",
    "print('SVM Accuracy',cross_val_score(svm_best,gmm_train, train_labels.values.ravel(), cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_best.fit(gmm_train,train_labels.values.ravel())\n",
    "pred  = rfc_best.predict(gmm_test)\n",
    "rfc_best_pred = pd.DataFrame(pred)\n",
    "\n",
    "rfc_best_pred.index += 1\n",
    "\n",
    "rfc_best_pred.columns = ['Solution']\n",
    "rfc_best_pred['Id'] = np.arange(1,rfc_best_pred.shape[0]+1)\n",
    "rfc_best_pred = rfc_best_pred[['Id', 'Solution']]\n",
    "\n",
    "rfc_best_pred.to_csv('Submission_GMM_RFC.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
